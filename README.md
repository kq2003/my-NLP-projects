Here are my machine learning projects / researches over the past few semesters. 
- Kaggle: CommonLit - Evaluate Student Summaries:
  This is our code for training models for kaggle competition: CommonLit - Evaluate Student Summaries. We cleaned the data, identified important parameters, and first tried out
  different pretrained models' performances on the dataset. We later stacked multiple pretrained huggingface pretrained models by building a stacking theme ourselves
  to reach an ideal model performance. We kept track of our model performance by cross-validation.
  
- TNEWS model performance recreation:
  This is my first time working with NLP tasks. I trained four Bert variations for TNEWS classification task (to classify what category each news belong to): Bert, Albert, 
  Roberta, and Ernie. I kept changing hyperparameters and train-test-split ratio to finally reach ideal model performance.
